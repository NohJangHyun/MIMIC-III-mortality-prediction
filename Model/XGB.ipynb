{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1641,
   "id": "dc96886b-f25b-418d-9fd7-825bd5721f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.preprocessing as prep\n",
    "import sklearn.model_selection as ms\n",
    "from functools import partial\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1852,
   "id": "0d200835-b119-4d6f-bc7e-be9aea704911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape\n",
      " (2527, 130) \n",
      "\n",
      "train shape\n",
      " (1421, 130) \n",
      "\n",
      "train value\n",
      " 0    1378\n",
      "1    1149\n",
      "Name: y, dtype: int64\n",
      "test value\n",
      " 0    1037\n",
      "1     384\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "train_data = pd.read_csv(\"~/project/MIMIC-III/Baseline/overlap/OSI/overlap_OSI(50%)_train.csv\")\n",
    "test_data = pd.read_csv(\"~/project/MIMIC-III/Baseline/overlap/OSI/overlap_OSI(50%)_test.csv\")\n",
    "\n",
    "# Overlap ARDS\n",
    "ARDS = pd.read_csv(\"~/project/MIMIC-III/ARDS.csv\")\n",
    "ARDS.drop(\"y\", axis=1, inplace=True)\n",
    "\n",
    "train_data = pd.merge(train_data, ARDS, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "test_data = pd.merge(test_data, ARDS, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "\n",
    "train_data.drop([\"SUBJECT_ID\",\"HADM_ID\", \"DOA\", \"TLOS\", \"ETHNICITY\"], axis=1, inplace=True)\n",
    "test_data.drop([\"SUBJECT_ID\",\"HADM_ID\", \"DOA\", \"TLOS\", \"ETHNICITY\"], axis=1, inplace=True)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=[\"GENDER\"])\n",
    "test_data = pd.get_dummies(test_data, columns=[\"GENDER\"])\n",
    "\n",
    "# train_data와 test_data의 칼럼을 맞추기 위해 차집합을 계산\n",
    "missing_columns_in_test = set(train_data.columns) - set(test_data.columns)\n",
    "missing_columns_in_train = set(test_data.columns) - set(train_data.columns)\n",
    "\n",
    "# test_data에 train_data의 더미 변수를 추가하고 0으로 채워주기\n",
    "for col in missing_columns_in_test:\n",
    "    test_data[col] = 0\n",
    "\n",
    "# train_data에 test_data의 더미 변수를 추가하고 0으로 채워주기\n",
    "for col in missing_columns_in_train:\n",
    "    train_data[col] = 0\n",
    "    \n",
    "####################################################################\n",
    "# Down Sampling\n",
    "\n",
    "# n:1\n",
    "# train 데이터의 레이블 비율 확인\n",
    "train_labels = train_data['y']\n",
    "label_counts = train_labels.value_counts()\n",
    "\n",
    "# train 데이터의 레이블 비율이 n:1이 되도록 분할\n",
    "num_samples_class_0 = int(label_counts[1] * 1.2)  # 레이블 0을 레이블 1의 n배로 설정\n",
    "num_samples_class_1 = label_counts[1]  # 레이블 1의 모든 샘플 개수\n",
    "\n",
    "class_0_data = train_data[train_data['y'] == 0]\n",
    "class_1_data = train_data[train_data['y'] == 1]\n",
    "\n",
    "# 레이블이 1인 데이터 중에서 샘플 개수가 적은 만큼만 선택\n",
    "class_1_data = class_1_data.sample(num_samples_class_1, random_state=42)\n",
    "\n",
    "# 레이블이 0인 데이터 중에서 샘플 개수가 적은 만큼만 선택\n",
    "class_0_data = class_0_data.sample(num_samples_class_0, random_state=42)\n",
    "\n",
    "# 선택한 데이터를 결합하여 최종 train 데이터 생성\n",
    "train_data = pd.concat([class_0_data, class_1_data])\n",
    "\n",
    "\n",
    "# # 1:1\n",
    "# # train 데이터의 레이블 비율 확인\n",
    "# train_labels = train_data['y']\n",
    "# label_counts = train_labels.value_counts()\n",
    "\n",
    "# # train 데이터의 레이블 비율이 1:1이 되도록 분할\n",
    "# class_0_data = train_data[train_data['y'] == 0]\n",
    "# class_1_data = train_data[train_data['y'] == 1]\n",
    "\n",
    "# # 레이블이 0인 데이터 중에서 샘플 개수가 적은 만큼만 선택\n",
    "# num_samples = min(label_counts[0], label_counts[1])\n",
    "# class_0_data = class_0_data.sample(num_samples, random_state=42)\n",
    "\n",
    "# # 레이블이 1인 데이터 중에서 샘플 개수가 적은 만큼만 선택\n",
    "# class_1_data = class_1_data.sample(num_samples, random_state=42)\n",
    "\n",
    "# # 선택한 데이터를 결합하여 최종 train 데이터 생성\n",
    "# train_data = pd.concat([class_0_data, class_1_data])\n",
    "####################################################################\n",
    "\n",
    "\n",
    "# Features와 Target 설정\n",
    "X_train = train_data.drop('y', axis=1)\n",
    "y_train = train_data['y']\n",
    "X_test = test_data.drop('y', axis=1)\n",
    "\n",
    "print(\"train shape\\n\",X_train.shape,\"\\n\")\n",
    "print(\"train shape\\n\",X_test.shape,\"\\n\")\n",
    "\n",
    "target = \"y\"\n",
    "features = [f for f in train_data.columns if f not in [target]]\n",
    "\n",
    "print(\"train value\\n\",train_data['y'].value_counts())\n",
    "print(\"test value\\n\",test_data['y'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1853,
   "id": "3450d603-4b5d-4400-bd46-2458d1ff2023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "칼럼 차이: set()\n"
     ]
    }
   ],
   "source": [
    "# test_data와 train_data의 칼럼 차이 확인\n",
    "column_difference = set(X_train.columns) - set(X_test.columns)\n",
    "print(\"칼럼 차이:\", column_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1854,
   "id": "b978230d-b165-41e5-90e0-ea4df87cc0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=0, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor='gpu_predictor', random_state=None, ...)"
      ]
     },
     "execution_count": 1854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=xgb.XGBClassifier(seed=1,\n",
    "            tree_method=\"gpu_hist\",\n",
    "            gpu_id=0,\n",
    "            predictor=\"gpu_predictor\",\n",
    "            n_estimators=100)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1855,
   "id": "d6fbcaf7-ba9e-4765-99ab-2c42f04946be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 71.01\n",
      "Precision: 47.01\n",
      "Recall: 57.29\n",
      "F1-score: 51.64\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(train_data['y'], y_pred2)\n",
    "print(\"Train Accuracy:\", accuracy) \n",
    "\n",
    "# Precision, Recall, F1-score 계산\n",
    "precision = precision_score(test_data['y'], y_pred)\n",
    "recall = recall_score(test_data['y'], y_pred)\n",
    "f1 = f1_score(test_data['y'], y_pred)\n",
    "accuracy = accuracy_score(test_data['y'], y_pred)\n",
    "\n",
    "# print(\"Test Accuracy:\", accuracy)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F1-score:\", f1)\n",
    "\n",
    "rounded_accuracy = round(accuracy * 100, 2)\n",
    "rounded_precision = round(precision * 100, 2)\n",
    "rounded_recall = round(recall * 100, 2)\n",
    "rounded_f1 = round(f1 * 100, 2)\n",
    "\n",
    "print(\"Test Accuracy:\", rounded_accuracy)\n",
    "print(\"Precision:\", rounded_precision)\n",
    "print(\"Recall:\", rounded_recall)\n",
    "print(\"F1-score:\", rounded_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1856,
   "id": "424ced4c-704b-4638-8fd5-0f3a8c4cf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidation:\n",
    "    def __init__(self, df, shuffle,random_state=None):\n",
    "        self.df = df\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "        if shuffle is True:\n",
    "            self.df = df.sample(frac=1,\n",
    "                random_state=self.random_state).reset_index(drop=True)\n",
    "\n",
    "    def hold_out_split(self,percent,stratify=None):\n",
    "        if stratify is not None:\n",
    "            y = self.df[stratify]\n",
    "            train,val = ms.train_test_split(self.df, test_size=percent/100,\n",
    "                stratify=y, random_state=self.random_state)\n",
    "            return train,val\n",
    "        size = len(self.df) - int(len(self.df)*(percent/100))\n",
    "        train = self.df.iloc[:size,:]\n",
    "        val = self.df.iloc[size:,:]\n",
    "        return train,val\n",
    "\n",
    "    def kfold_split(self, splits, stratify=None):\n",
    "        if stratify is not None:\n",
    "            kf = ms.StratifiedKFold(n_splits=splits,\n",
    "                shuffle=self.shuffle,\n",
    "                random_state=self.random_state)\n",
    "            y = self.df[stratify]\n",
    "            for train, val in kf.split(X=self.df,y=y):\n",
    "                t = self.df.iloc[train,:]\n",
    "                v = self.df.iloc[val, :]\n",
    "                yield t,v\n",
    "        else:\n",
    "            kf = ms.KFold(n_splits=splits, shuffle=self.shuffle,\n",
    "                random_state=self.random_state)\n",
    "            for train, val in kf.split(X=self.df):\n",
    "                t = self.df.iloc[train,:]\n",
    "                v = self.df.iloc[val, :]\n",
    "                yield t,v\n",
    "seed = 42\n",
    "folds = 5\n",
    "cv = CrossValidation(train_data,\n",
    "                     shuffle=True,\n",
    "                     random_state=seed\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1857,
   "id": "a433f517-2a23-43c7-9512-680605da6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_params(trial):\n",
    "    # gamma = trial.suggest_categorical(\"gamma\", [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 204.8])\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=50)\n",
    "    # learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.4, step=0.01)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 100.0, log=True)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 100.0, log=True)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0, step=0.1)\n",
    "    max_depth = trial.suggest_categorical(\"max_depth\", [3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# # def tuning_params(trial):\n",
    "# #     gamma = trial.suggest_categorical(\"gamma\", [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 200])\n",
    "# #     subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n",
    "# #     colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)             \n",
    "# #     n_estimators = trial.suggest_categorical(\"n_estimators\", [50, 65, 80, 100, 115, 130, 150])\n",
    "# #     learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n",
    "# #     max_depth = trial.suggest_categorical(\"max_depth\", [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n",
    "# #     reg_alpha = trial.suggest_categorical(\"reg_alpha\", [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 200])\n",
    "# #     reg_lambda = trial.suggest_categorical(\"reg_lambda\", [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 200])\n",
    "    \n",
    "    \n",
    "# def tuning_params(trial):\n",
    "#     gamma = trial.suggest_categorical(\"gamma\", [i * 0.1 for i in range(1, 301)])\n",
    "#     n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=50)\n",
    "#     learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, step=0.01)\n",
    "#     reg_alpha = trial.suggest_categorical(\"reg_alpha\", [i * 0.1 for i in range(1, 301)])\n",
    "#     reg_lambda = trial.suggest_categorical(\"reg_lambda\", [i * 0.1 for i in range(1, 301)])\n",
    "#     subsample = trial.suggest_float(\"subsample\", 0.3, 1.0, step=0.1)\n",
    "#     colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.3, 1.0, step=0.1)\n",
    "#     max_depth = trial.suggest_categorical(\"max_depth\", [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n",
    "\n",
    "    for train_, val_ in cv.kfold_split(folds):\n",
    "        trainX = train_[features]\n",
    "        trainY = train_[target]\n",
    "        valX = val_[features]\n",
    "        valY = val_[target]\n",
    "        \n",
    "        model = xgb.XGBClassifier(\n",
    "            seed=1,\n",
    "            tree_method=\"gpu_hist\",\n",
    "            gpu_id=0,\n",
    "            predictor=\"gpu_predictor\",\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            reg_lambda=reg_lambda,\n",
    "            reg_alpha=reg_alpha,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            max_depth=max_depth,\n",
    "            use_label_encoder=False,\n",
    "            early_stopping_rounds=300, \n",
    "                 \n",
    "        )\n",
    "        \n",
    "        model.fit(trainX, trainY, \n",
    "                   eval_set=[(valX, valY)],\n",
    "                  verbose=False\n",
    "                 )\n",
    "    \n",
    "        predY = model.predict(valX)\n",
    "        # val_f1 = f1_score(valY, predY)# Calculate F1-score\n",
    "        # return val_f1\n",
    "    \n",
    "        val_auc = metrics.roc_auc_score(valY, predY)\n",
    "        return val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1858,
   "id": "3f2e6ee8-b7a7-404b-a771-a2c9c6a1ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 목표 F1 score 설정\n",
    "# target_f1_score = 0.72\n",
    "\n",
    "# # Optuna study 생성\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# # 목표 F1 score가 달성될 때까지 반복\n",
    "# while True:\n",
    "#     # 새로운 trial 시작\n",
    "#     study.optimize(tuning_params, n_trials=1, gc_after_trial=True)\n",
    "    \n",
    "#     # 현재의 최적 F1 score 가져오기\n",
    "#     best_f1_score = study.best_value\n",
    "    \n",
    "#     # 현재 F1 score가 목표 F1 score를 넘으면 반복 중단\n",
    "#     if best_f1_score >= target_f1_score:\n",
    "#         print(\"목표 F1 Score 달성!\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1859,
   "id": "b3dfac61-129d-4b81-ab0a-edcc4366335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-09-06 15:30:25,056]\u001b[0m A new study created in memory with name: no-name-34f44721-2317-44a1-bf58-d2e9733b88ea\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:26,642]\u001b[0m Trial 0 finished with value: 0.6453956961335705 and parameters: {'n_estimators': 1450, 'learning_rate': 0.1547991814894585, 'reg_lambda': 0.10636422411501283, 'reg_alpha': 1.2709826183496653, 'subsample': 0.6, 'colsample_bytree': 1.0, 'max_depth': 10}. Best is trial 0 with value: 0.6453956961335705.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:28,099]\u001b[0m Trial 1 finished with value: 0.6665144450319666 and parameters: {'n_estimators': 550, 'learning_rate': 0.07233699169591834, 'reg_lambda': 0.002406443096048235, 'reg_alpha': 4.5843601073436465e-08, 'subsample': 0.1, 'colsample_bytree': 0.9, 'max_depth': 9}. Best is trial 1 with value: 0.6665144450319666.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:29,523]\u001b[0m Trial 2 finished with value: 0.6513643865468123 and parameters: {'n_estimators': 1750, 'learning_rate': 0.09559813222179415, 'reg_lambda': 7.332790837311976e-05, 'reg_alpha': 0.04404287462902575, 'subsample': 0.1, 'colsample_bytree': 0.30000000000000004, 'max_depth': 5}. Best is trial 1 with value: 0.6665144450319666.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:30,461]\u001b[0m Trial 3 finished with value: 0.6556346039833998 and parameters: {'n_estimators': 150, 'learning_rate': 0.10972101679769326, 'reg_lambda': 1.3286319218201098e-08, 'reg_alpha': 9.189673290707829e-08, 'subsample': 0.4, 'colsample_bytree': 1.0, 'max_depth': 8}. Best is trial 1 with value: 0.6665144450319666.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:32,788]\u001b[0m Trial 4 finished with value: 0.6887067570382478 and parameters: {'n_estimators': 650, 'learning_rate': 0.04070385938874429, 'reg_lambda': 6.5004839920126515e-06, 'reg_alpha': 1.2766558378949189, 'subsample': 1.0, 'colsample_bytree': 0.8, 'max_depth': 8}. Best is trial 4 with value: 0.6887067570382478.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:36,913]\u001b[0m Trial 5 finished with value: 0.7153695781057219 and parameters: {'n_estimators': 950, 'learning_rate': 0.04214294305326099, 'reg_lambda': 57.69855688865746, 'reg_alpha': 1.6532869074675893, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.5, 'max_depth': 9}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:38,861]\u001b[0m Trial 6 finished with value: 0.6857103943341505 and parameters: {'n_estimators': 150, 'learning_rate': 0.04692458796698439, 'reg_lambda': 1.8771038319439923, 'reg_alpha': 1.448175208060846, 'subsample': 0.2, 'colsample_bytree': 0.5, 'max_depth': 8}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:43,208]\u001b[0m Trial 7 finished with value: 0.6906215449694756 and parameters: {'n_estimators': 1250, 'learning_rate': 0.041494107189745603, 'reg_lambda': 3.914015597716702e-07, 'reg_alpha': 2.7372153504955757e-05, 'subsample': 0.2, 'colsample_bytree': 0.9, 'max_depth': 10}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:44,981]\u001b[0m Trial 8 finished with value: 0.7044897370571552 and parameters: {'n_estimators': 150, 'learning_rate': 0.03510969638939812, 'reg_lambda': 1.7674263120554538e-08, 'reg_alpha': 2.422957190055806e-08, 'subsample': 0.6, 'colsample_bytree': 0.4, 'max_depth': 7}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:49,579]\u001b[0m Trial 9 finished with value: 0.6577576952042173 and parameters: {'n_estimators': 1650, 'learning_rate': 0.06772358220499229, 'reg_lambda': 0.03672080580344583, 'reg_alpha': 0.9326867392136851, 'subsample': 1.0, 'colsample_bytree': 0.1, 'max_depth': 10}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:54,288]\u001b[0m Trial 10 finished with value: 0.5 and parameters: {'n_estimators': 950, 'learning_rate': 0.014510005200889207, 'reg_lambda': 82.83898383342826, 'reg_alpha': 63.80481821357112, 'subsample': 0.4, 'colsample_bytree': 0.7000000000000001, 'max_depth': 9}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:30:59,048]\u001b[0m Trial 11 finished with value: 0.6801663221650724 and parameters: {'n_estimators': 850, 'learning_rate': 0.026433119955961922, 'reg_lambda': 0.00032844371041582007, 'reg_alpha': 0.0002867469813910968, 'subsample': 0.6, 'colsample_bytree': 0.5, 'max_depth': 7}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:03,985]\u001b[0m Trial 12 finished with value: 0.6908298482590652 and parameters: {'n_estimators': 2000, 'learning_rate': 0.025695354371842685, 'reg_lambda': 1.2837846319961727e-08, 'reg_alpha': 2.009947605285971e-06, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.30000000000000004, 'max_depth': 7}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:06,512]\u001b[0m Trial 13 finished with value: 0.6882741271291 and parameters: {'n_estimators': 450, 'learning_rate': 0.1962510250182754, 'reg_lambda': 4.61120059591569e-06, 'reg_alpha': 0.0021443252277613526, 'subsample': 0.8, 'colsample_bytree': 0.30000000000000004, 'max_depth': 3}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:13,832]\u001b[0m Trial 14 finished with value: 0.7053389735454821 and parameters: {'n_estimators': 1150, 'learning_rate': 0.011937076997712764, 'reg_lambda': 78.0256710982993, 'reg_alpha': 1.1454883738452062e-08, 'subsample': 0.4, 'colsample_bytree': 0.6, 'max_depth': 4}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:16,681]\u001b[0m Trial 15 finished with value: 0.7053389735454821 and parameters: {'n_estimators': 1200, 'learning_rate': 0.01133501490935782, 'reg_lambda': 90.04794071401099, 'reg_alpha': 1.9804534013243604e-06, 'subsample': 0.4, 'colsample_bytree': 0.7000000000000001, 'max_depth': 4}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:19,295]\u001b[0m Trial 16 finished with value: 0.700644137864731 and parameters: {'n_estimators': 1100, 'learning_rate': 0.016895055315360234, 'reg_lambda': 6.325265215689434, 'reg_alpha': 0.0037198729568781747, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.6, 'max_depth': 4}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:21,756]\u001b[0m Trial 17 finished with value: 0.7002195196205676 and parameters: {'n_estimators': 750, 'learning_rate': 0.01934776791525525, 'reg_lambda': 3.3243655745319054, 'reg_alpha': 8.884678043303712e-05, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.1, 'max_depth': 6}. Best is trial 5 with value: 0.7153695781057219.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:25,178]\u001b[0m Trial 18 finished with value: 0.7262494191542888 and parameters: {'n_estimators': 1350, 'learning_rate': 0.010113376756694981, 'reg_lambda': 0.38736455161296907, 'reg_alpha': 0.016227597759968154, 'subsample': 0.5, 'colsample_bytree': 0.6, 'max_depth': 9}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:28,355]\u001b[0m Trial 19 finished with value: 0.6955246839398165 and parameters: {'n_estimators': 1400, 'learning_rate': 0.010717760481366804, 'reg_lambda': 0.1899939981416951, 'reg_alpha': 0.028607430372360524, 'subsample': 0.8, 'colsample_bytree': 0.4, 'max_depth': 9}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:31,152]\u001b[0m Trial 20 finished with value: 0.680374625454662 and parameters: {'n_estimators': 1500, 'learning_rate': 0.023817095016264198, 'reg_lambda': 0.014568759942297227, 'reg_alpha': 43.32416511309165, 'subsample': 0.5, 'colsample_bytree': 0.7000000000000001, 'max_depth': 9}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:33,721]\u001b[0m Trial 21 finished with value: 0.7170760627473601 and parameters: {'n_estimators': 1000, 'learning_rate': 0.013290302914976267, 'reg_lambda': 12.27981976168134, 'reg_alpha': 0.006233195432854935, 'subsample': 0.5, 'colsample_bytree': 0.6, 'max_depth': 4}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:36,733]\u001b[0m Trial 22 finished with value: 0.7100338092262333 and parameters: {'n_estimators': 950, 'learning_rate': 0.01417657809953575, 'reg_lambda': 0.6645464058002709, 'reg_alpha': 0.02109933197148483, 'subsample': 0.5, 'colsample_bytree': 0.6, 'max_depth': 9}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:40,101]\u001b[0m Trial 23 finished with value: 0.7023506225063694 and parameters: {'n_estimators': 1300, 'learning_rate': 0.010293311586520013, 'reg_lambda': 8.441986672415737, 'reg_alpha': 0.16956548521611362, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.4, 'max_depth': 5}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:42,118]\u001b[0m Trial 24 finished with value: 0.70960919098207 and parameters: {'n_estimators': 950, 'learning_rate': 0.019470478196685702, 'reg_lambda': 0.39235386654304577, 'reg_alpha': 0.0038618905423481583, 'subsample': 0.5, 'colsample_bytree': 0.5, 'max_depth': 3}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:43,712]\u001b[0m Trial 25 finished with value: 0.7072617731416944 and parameters: {'n_estimators': 400, 'learning_rate': 0.015153636999132613, 'reg_lambda': 13.740076856800528, 'reg_alpha': 0.0012816562269168939, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.8, 'max_depth': 6}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:45,000]\u001b[0m Trial 26 finished with value: 0.691046163213639 and parameters: {'n_estimators': 800, 'learning_rate': 0.032132441223558544, 'reg_lambda': 0.8202863239382664, 'reg_alpha': 0.1091693952671415, 'subsample': 0.2, 'colsample_bytree': 0.6, 'max_depth': 4}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:48,839]\u001b[0m Trial 27 finished with value: 0.7151612748161323 and parameters: {'n_estimators': 1550, 'learning_rate': 0.02031934526382918, 'reg_lambda': 18.19921208935028, 'reg_alpha': 0.011515373161839428, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.7000000000000001, 'max_depth': 9}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:53,222]\u001b[0m Trial 28 finished with value: 0.7091845727379065 and parameters: {'n_estimators': 1850, 'learning_rate': 0.013245120237335596, 'reg_lambda': 1.6435950712723866, 'reg_alpha': 0.0005698855413268769, 'subsample': 0.5, 'colsample_bytree': 0.5, 'max_depth': 9}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n",
      "\u001b[32m[I 2023-09-06 15:31:56,515]\u001b[0m Trial 29 finished with value: 0.7055552885000561 and parameters: {'n_estimators': 1350, 'learning_rate': 0.017540681327020864, 'reg_lambda': 0.09346384015459491, 'reg_alpha': 8.590490759464528, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.2, 'max_depth': 4}. Best is trial 18 with value: 0.7262494191542888.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(tuning_params, n_trials=30, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1860,
   "id": "be4f0ba9-1c0a-42e9-a5c3-486201916197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.7262494191542888\n",
      "Best Hyperparameters: {'n_estimators': 1350, 'learning_rate': 0.010113376756694981, 'reg_lambda': 0.38736455161296907, 'reg_alpha': 0.016227597759968154, 'subsample': 0.5, 'colsample_bytree': 0.6, 'max_depth': 9}\n"
     ]
    }
   ],
   "source": [
    "# 최적의 F1 스코어 및 하이퍼파라미터 출력\n",
    "print(\"Best F1 Score:\", study.best_value)\n",
    "print(\"Best Hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1861,
   "id": "c3defb2e-af79-4efe-9716-f5b7da240ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_, val_ in cv.kfold_split(folds):\n",
    "        trainX = train_[features]\n",
    "        trainY = train_[target]\n",
    "        valX = val_[features]\n",
    "        valY = val_[target]\n",
    "        \n",
    "        model = xgb.XGBClassifier(\n",
    "            seed=1,\n",
    "            tree_method=\"gpu_hist\",\n",
    "            gpu_id=0,\n",
    "            predictor=\"gpu_predictor\",\n",
    "            # gamma = study.best_params['gamma'],\n",
    "            n_estimators=study.best_params['n_estimators'],\n",
    "            learning_rate=study.best_params['learning_rate'],\n",
    "            reg_lambda=study.best_params['reg_lambda'],\n",
    "            reg_alpha=study.best_params['reg_alpha'],\n",
    "            subsample=study.best_params['subsample'],\n",
    "            colsample_bytree=study.best_params['colsample_bytree'],\n",
    "            max_depth=study.best_params['max_depth'],\n",
    "            use_label_encoder=False,\n",
    "        )\n",
    "        model.fit(trainX,trainY,\n",
    "                 early_stopping_rounds=500, \n",
    "                          eval_set=[(valX, valY)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1862,
   "id": "83d9575c-6b54-4c42-bb1f-b459ff0272c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 72.62\n",
      "Precision: 49.37\n",
      "Recall: 51.04\n",
      "F1-score: 50.19\n",
      "AUROC: 65.83\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(test_data['y'], y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(test_data['y'], y_pred)\n",
    "\n",
    "# Precision, Recall, F1-score, AUROC 계산\n",
    "precision = precision_score(test_data['y'], y_pred)\n",
    "recall = recall_score(test_data['y'], y_pred)\n",
    "f1 = f1_score(test_data['y'], y_pred)\n",
    "accuracy = accuracy_score(test_data['y'], y_pred)\n",
    "auroc = roc_auc_score(test_data['y'], y_pred)\n",
    "\n",
    "rounded_accuracy = round(accuracy * 100, 2)\n",
    "rounded_precision = round(precision * 100, 2)\n",
    "rounded_recall = round(recall * 100, 2)\n",
    "rounded_f1 = round(f1 * 100, 2)\n",
    "rounded_auroc = round(auroc * 100, 2)\n",
    "\n",
    "print(\"Test Accuracy:\", rounded_accuracy)\n",
    "print(\"Precision:\", rounded_precision)\n",
    "print(\"Recall:\", rounded_recall)\n",
    "print(\"F1-score:\", rounded_f1)\n",
    "print(\"AUROC:\", rounded_auroc)\n",
    "\n",
    "# # Confusion Matrix 시각화\n",
    "# cm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.ylabel('Actual Label')\n",
    "# plt.show()\n",
    "\n",
    "# # Classification Report 출력\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(test_data['y'], y_pred))\n",
    "\n",
    "# # Precision-Recall Curve 그리기\n",
    "# precision, recall, _ = precision_recall_curve(test_data['y'], y_pred)\n",
    "# average_precision = average_precision_score(test_data['y'], y_pred)\n",
    "# area_under_curve = auc(recall, precision)\n",
    "\n",
    "# # Precision-Recall Curve 시각화\n",
    "# plt.figure()\n",
    "# plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "# plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title(f'Precision-Recall Curve (AP = {average_precision:.2f})')\n",
    "# # plt.title(f'Precision-Recall Curve (AUC = {area_under_curve:.2f})')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1607,
   "id": "d19b7141-4a03-4f8a-829d-4e8b7e896fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance = model.feature_importances_\n",
    "\n",
    "# # Get feature names\n",
    "# feature_names = X_train.columns\n",
    "\n",
    "# # Create a dictionary to store feature importance values and names\n",
    "# feature_importance_dict = {feature: importance for feature, importance in zip(feature_names, feature_importance)}\n",
    "\n",
    "# # Sort the feature importance dictionary by importance values in descending order\n",
    "# sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Get the top 10 important features\n",
    "# top_10_features = sorted_feature_importance[:10]\n",
    "\n",
    "# # Extract feature names and importance values from the top 10 features\n",
    "# top_10_feature_names = [feature[0] for feature in top_10_features]\n",
    "# top_10_feature_importance = [feature[1] for feature in top_10_features]\n",
    "\n",
    "# # Invert the lists to reverse the order\n",
    "# top_10_feature_names.reverse()\n",
    "# top_10_feature_importance.reverse()\n",
    "\n",
    "# # Create a bar plot for the top 10 important features\n",
    "# plt.barh(top_10_feature_names, top_10_feature_importance)\n",
    "# plt.xlabel('Importance')\n",
    "# plt.ylabel('Features')\n",
    "# plt.title('Top 10 Important Features')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1562,
   "id": "acdd6c16-8349-41c7-801e-660101aad87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred2 = model.predict(X_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(train_data['y'], y_pred2)\n",
    "# print(\"Train Accuracy:\", accuracy)\n",
    "# accuracy = accuracy_score(test_data['y'], y_pred)\n",
    "# print(\"Test Accuracy:\", accuracy)\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# cm = confusion_matrix(test_data['y'], y_pred)\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Confusion matrix를 pandas DataFrame으로 변환합니다\n",
    "# cm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "\n",
    "# # Confusion matrix를 시각화합니다\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.ylabel('Actual Label')\n",
    "# plt.show()\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(test_data['y'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da47ea-c8da-4782-838e-25f8010f7acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed990556-a890-452b-ad89-f7a63471cea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
